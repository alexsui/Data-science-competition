{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "fa1b0e02-7caf-4e30-b82d-9c1882e3b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.models import resnet50\n",
    "import torchvision.models as models\n",
    "# from torchsummary import summary\n",
    "import torchinfo as summary\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class depthwise_separable_conv(nn.Module):\n",
    "    def __init__(self, nin, nout,kernel_size =3 , stride=1, padding=0): \n",
    "        super(depthwise_separable_conv, self).__init__() \n",
    "        self.depthwise = nn.Conv2d(nin, nin, kernel_size, stride = stride, padding = padding, groups=nin) \n",
    "        self.pointwise = nn.Conv2d(nin, nout, 1) \n",
    "\n",
    "    def forward(self, x): \n",
    "        out = self.depthwise(x) \n",
    "        out = self.pointwise(out) \n",
    "        return out\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            depthwise_separable_conv(3,64,padding=\"same\"),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            depthwise_separable_conv(64,64),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            depthwise_separable_conv(64,128,padding=\"same\"),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            depthwise_separable_conv(128,128),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            depthwise_separable_conv(128,256,padding=\"same\"),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            depthwise_separable_conv(256,256),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            #  depthwise_separable_conv(64,256,padding=\"same\"),\n",
    "            # nn.BatchNorm2d(256),\n",
    "            # nn.ReLU(),\n",
    "            # depthwise_separable_conv(256,256,padding=\"same\"),\n",
    "            # nn.BatchNorm2d(256),\n",
    "            # nn.ReLU(),\n",
    "            depthwise_separable_conv(256,256,padding=\"same\"),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            depthwise_separable_conv(256,512,padding=\"same\"),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((1,1)), # [128,1,1]\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256,10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "663d01f2-f0dc-4a58-8dbe-925500bf10dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def studentLossFn(teacher_pred, student_pred, y, T=3, alpha=0.4):\n",
    "    if (alpha > 0):\n",
    "        loss = F.kl_div(F.log_softmax(student_pred / T, dim=1), F.softmax(teacher_pred / T, dim=1), reduction='batchmean') * (T ** 2) * alpha + F.cross_entropy(student_pred, y) * (1 - alpha)\n",
    "    else:\n",
    "        loss = F.cross_entropy(student_pred, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "92f5ac87-5758-4bbe-abf4-f7b0481e534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(ResNet, self).__init__()\n",
    "            self.resnet50 = resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "            num_ftrs = self.resnet50.fc.in_features\n",
    "            self.resnet50.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.resnet50.conv1(x)\n",
    "            x = self.resnet50.bn1(x)\n",
    "            x = self.resnet50.relu(x)\n",
    "            x = self.resnet50.maxpool(x)\n",
    "\n",
    "            x = self.resnet50.layer1(x)\n",
    "            x = self.resnet50.layer2(x)\n",
    "            x = self.resnet50.layer3(x)\n",
    "            x = self.resnet50.layer4(x)\n",
    "\n",
    "            x = self.resnet50.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.resnet50.fc(x)\n",
    "            return x\n",
    "\n",
    "teacher_model = ResNet()\n",
    "weights_path = './resnet-50.pth'\n",
    "checkpoint = torch.load(weights_path)\n",
    "teacher_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "teacher_model = teacher_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "957da1b2-da68-4e45-a0a2-bc70dd72554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "        [transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))])\n",
    "test_transform = transforms.Compose(\n",
    "        [transforms.Grayscale(num_output_channels=3),  # gray to 3 channel\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
    "                                                download=True, transform=train_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                            shuffle=True, num_workers=8)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
    "                                            download=True, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
    "                                            shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "7fdff4b3-afce-4f21-b82e-cd64925d49eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(student_model,teacher_model, train_dataloader,train_loss_function, opt, device):\n",
    "    train_acc = 0\n",
    "    train_loss = 0\n",
    "    student_model.train()\n",
    "    for x,y in train_dataloader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        student_pred = student_model(x)\n",
    "        with torch.no_grad():\n",
    "            teacher_pred = teacher_model(x)\n",
    "        loss = train_loss_function(teacher_pred,student_pred, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (torch.max(student_pred,1)[1]==y).sum().item()/len(student_pred)\n",
    "    return train_loss/len(train_dataloader), train_acc/len(train_dataloader)\n",
    "\n",
    "def test_step(student_model, test_dataloader, test_loss_function, device): \n",
    "    test_acc = 0\n",
    "    test_loss = 0\n",
    "    student_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for x,y in test_dataloader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            pred_y = student_model(x)\n",
    "            # print(\"pred_y:\",pred_y.shape)\n",
    "            loss = test_loss_function(pred_y,y) \n",
    "            test_loss += loss.item()\n",
    "            test_acc += (torch.argmax(pred_y,1)==y).sum().item()/len(pred_y)\n",
    "    return test_loss/len(test_dataloader), test_acc/len(test_dataloader)\n",
    "def train(epochs, student_model, teacher_model,\n",
    "          train_dataloader, test_dataloader, opt, \n",
    "          train_loss_function,test_loss_function,\n",
    "          device, patience,model_name):\n",
    "    last_loss = float(\"inf\")\n",
    "    cur = 0\n",
    "    results ={\n",
    "        \"train_loss\":[],\n",
    "        \"train_acc\":[],\n",
    "        \"test_loss\":[],\n",
    "        \"test_acc\":[]\n",
    "    }\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(student_model=student_model,\n",
    "                                           teacher_model = teacher_model,\n",
    "                                          train_dataloader=train_dataloader,\n",
    "                                          train_loss_function=train_loss_function,\n",
    "                                          opt=opt,\n",
    "                                          device=device)\n",
    "        test_loss, test_acc = test_step(student_model=student_model,\n",
    "                                          test_dataloader=test_dataloader,\n",
    "                                          test_loss_function=test_loss_function,\n",
    "                                          device=device)\n",
    "        if test_loss > last_loss:\n",
    "            cur += 1\n",
    "            print('trigger times:', cur)\n",
    "            if cur >= patience:\n",
    "                print(\"early stop !\")\n",
    "                return results\n",
    "        else:\n",
    "            cur = 0\n",
    "        last_loss = test_loss\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "      # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        if (epoch+1)%10 == 0:\n",
    "            MODEL_PATH = Path(model_name)\n",
    "            MODEL_PATH.mkdir(parents=True, # create parent directories if needed\n",
    "                             exist_ok=True # if models directory already exists, don't error\n",
    "            )\n",
    "\n",
    "            # Create model save path\n",
    "            MODEL_NAME = f\"model_{epoch+1}.pth\"\n",
    "            MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "            # Save the model state dict\n",
    "            print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "            torch.save(obj=student_model.state_dict(), # only saving the state_dict() only saves the learned parameters\n",
    "                       f=MODEL_SAVE_PATH)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c56c6-b00d-46fa-907c-f13906f2922e",
   "metadata": {},
   "source": [
    "# Prune model (假設已經train好準備要pruned model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "95db4812-be4f-4cfb-80c9-7f4cce8296ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "import torchinfo as summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "bd5a0536-8823-41f4-b74b-c9514e121824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_net = StudentModel()\n",
    "checkpoint = torch.load(\"model_to_prune.pth\")\n",
    "pruned_net.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d0342104-5b1a-4691-82fc-cf1a83c1d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the module to prune\n",
    "param_to_prune=[]\n",
    "for i,(name, module) in enumerate(pruned_net.named_modules()):\n",
    "    if i>=2:\n",
    "        if isinstance(module,depthwise_separable_conv):\n",
    "            param_to_prune.append((module.depthwise,'weight'))\n",
    "            param_to_prune.append((module.pointwise,'weight'))\n",
    "        elif isinstance(module,nn.Conv2d)|isinstance(module,nn.Linear)|isinstance(module,nn.BatchNorm2d):\n",
    "            param_to_prune.append((module,'weight'))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "7218a09c-c07a-4fbd-9549-5c2b358280c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.global_unstructured(\n",
    "    param_to_prune,\n",
    "     pruning_method = prune.L1Unstructured,\n",
    "     amount = 0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f19ccb8-e914-4075-af78-d6a04444e82b",
   "metadata": {},
   "source": [
    "### _forward_pre_hooks 確保 model在做 forward前會mask 掉pruning weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "4eb8b01c-80ff-4554-a49c-5d4562cb2a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=3) OrderedDict([(570, <torch.nn.utils.prune.PruningContainer object at 0x7efa4d7f9700>)])\n",
      "----------\n",
      "Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1)) OrderedDict([(571, <torch.nn.utils.prune.PruningContainer object at 0x7efa4d83b730>)])\n",
      "----------\n",
      "Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=3) OrderedDict([(570, <torch.nn.utils.prune.PruningContainer object at 0x7efa4d7f9700>)])\n",
      "----------\n",
      "Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1)) OrderedDict([(571, <torch.nn.utils.prune.PruningContainer object at 0x7efa4d83b730>)])\n",
      "----------\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) OrderedDict([(572, <torch.nn.utils.prune.CustomFromMask object at 0x7efa4d83b790>)])\n",
      "----------\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), groups=64) OrderedDict([(575, <torch.nn.utils.prune.PruningContainer object at 0x7efa4d84cfa0>)])\n",
      "----------\n",
      "Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1)) OrderedDict([(576, <torch.nn.utils.prune.PruningContainer object at 0x7efa4d84c280>)])\n",
      "----------\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), groups=64) OrderedDict([(575, <torch.nn.utils.prune.PruningContainer object at 0x7efa4d84cfa0>)])\n",
      "----------\n",
      "Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1)) OrderedDict([(576, <torch.nn.utils.prune.PruningContainer object at 0x7efa4d84c280>)])\n",
      "----------\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) OrderedDict([(577, <torch.nn.utils.prune.CustomFromMask object at 0x7efa4d84c220>)])\n",
      "----------\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=64) OrderedDict([(580, <torch.nn.utils.prune.PruningContainer object at 0x7efa4d84c6d0>)])\n",
      "----------\n",
      "Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1)) OrderedDict([(581, <torch.nn.utils.prune.PruningContainer object at 0x7efa4d83c0d0>)])\n",
      "----------\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=64) OrderedDict([(580, <torch.nn.utils.prune.PruningContainer object at 0x7efa4d84c6d0>)])\n",
      "----------\n",
      "Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1)) OrderedDict([(581, <torch.nn.utils.prune.PruningContainer object at 0x7efa4d83c0d0>)])\n",
      "----------\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) OrderedDict([(582, <torch.nn.utils.prune.CustomFromMask object at 0x7efa4d83c820>)])\n",
      "----------\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128) OrderedDict([(585, <torch.nn.utils.prune.PruningContainer object at 0x7efb44e965b0>)])\n",
      "----------\n",
      "Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1)) OrderedDict([(586, <torch.nn.utils.prune.PruningContainer object at 0x7efaa1ce0970>)])\n",
      "----------\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128) OrderedDict([(585, <torch.nn.utils.prune.PruningContainer object at 0x7efb44e965b0>)])\n",
      "----------\n",
      "Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1)) OrderedDict([(586, <torch.nn.utils.prune.PruningContainer object at 0x7efaa1ce0970>)])\n",
      "----------\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) OrderedDict([(587, <torch.nn.utils.prune.CustomFromMask object at 0x7efaa1ce0a90>)])\n",
      "----------\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=128) OrderedDict([(590, <torch.nn.utils.prune.PruningContainer object at 0x7efaa1ce0a30>)])\n",
      "----------\n",
      "Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1)) OrderedDict([(591, <torch.nn.utils.prune.PruningContainer object at 0x7efaa1ce09a0>)])\n",
      "----------\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=128) OrderedDict([(590, <torch.nn.utils.prune.PruningContainer object at 0x7efaa1ce0a30>)])\n",
      "----------\n",
      "Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1)) OrderedDict([(591, <torch.nn.utils.prune.PruningContainer object at 0x7efaa1ce09a0>)])\n",
      "----------\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) OrderedDict([(592, <torch.nn.utils.prune.CustomFromMask object at 0x7efaa1ce0ca0>)])\n",
      "----------\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), groups=256) OrderedDict([(595, <torch.nn.utils.prune.PruningContainer object at 0x7efaa1ce0f40>)])\n",
      "----------\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1)) OrderedDict([(596, <torch.nn.utils.prune.PruningContainer object at 0x7efaa1ce0a00>)])\n",
      "----------\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), groups=256) OrderedDict([(595, <torch.nn.utils.prune.PruningContainer object at 0x7efaa1ce0f40>)])\n",
      "----------\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1)) OrderedDict([(596, <torch.nn.utils.prune.PruningContainer object at 0x7efaa1ce0a00>)])\n",
      "----------\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) OrderedDict([(597, <torch.nn.utils.prune.CustomFromMask object at 0x7efaa1ce0fa0>)])\n",
      "----------\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=256) OrderedDict([(600, <torch.nn.utils.prune.PruningContainer object at 0x7efa86fd97c0>)])\n",
      "----------\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1)) OrderedDict([(601, <torch.nn.utils.prune.PruningContainer object at 0x7efa86fd96d0>)])\n",
      "----------\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=256) OrderedDict([(600, <torch.nn.utils.prune.PruningContainer object at 0x7efa86fd97c0>)])\n",
      "----------\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1)) OrderedDict([(601, <torch.nn.utils.prune.PruningContainer object at 0x7efa86fd96d0>)])\n",
      "----------\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) OrderedDict([(602, <torch.nn.utils.prune.CustomFromMask object at 0x7efa86fd93a0>)])\n",
      "----------\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=256) OrderedDict([(605, <torch.nn.utils.prune.PruningContainer object at 0x7efa86fd9040>)])\n",
      "----------\n",
      "Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1)) OrderedDict([(606, <torch.nn.utils.prune.PruningContainer object at 0x7efa86fd9070>)])\n",
      "----------\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=256) OrderedDict([(605, <torch.nn.utils.prune.PruningContainer object at 0x7efa86fd9040>)])\n",
      "----------\n",
      "Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1)) OrderedDict([(606, <torch.nn.utils.prune.PruningContainer object at 0x7efa86fd9070>)])\n",
      "----------\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) OrderedDict([(607, <torch.nn.utils.prune.CustomFromMask object at 0x7efa86fd9700>)])\n",
      "----------\n",
      "Linear(in_features=512, out_features=256, bias=True) OrderedDict([(608, <torch.nn.utils.prune.CustomFromMask object at 0x7efa86fd9640>)])\n",
      "----------\n",
      "Linear(in_features=256, out_features=10, bias=True) OrderedDict([(609, <torch.nn.utils.prune.CustomFromMask object at 0x7efa8c67a070>)])\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for p in param_to_prune:\n",
    "    print(p[0],p[0]._forward_pre_hooks)\n",
    "    print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded3180-ea25-49e9-8d36-488585e13126",
   "metadata": {},
   "source": [
    "### Retrain pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e36163-8dd1-45ba-9ca4-0aba52d7820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 400\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d4c521-ed3c-482a-ad2c-ad159d5611e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(pruned_net.parameters(),lr = LR )\n",
    "pruned_net.to(device)\n",
    "results = train(EPOCH,\n",
    "      pruned_net,teacher_model,\n",
    "      trainloader,testloader,\n",
    "      train_loss_function = studentLossFn,\n",
    "      test_loss_function = nn.CrossEntropyLoss(),\n",
    "      opt = opt,\n",
    "      device=device,\n",
    "      patience = 5,\n",
    "      model_name = \"pruned_student_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b2fbf2-9847-40c9-ba41-b14ca9dbe729",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing retrained model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "8ab50c5f-19e1-45ad-b89c-05bcc2bde960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 93.47 %\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"pruned_student_model/model_170.pth\")\n",
    "pruned_net.load_state_dict(checkpoint)\n",
    "pruned_net.to(device)\n",
    "pruned_net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "pred_arr = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = pruned_net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        pred_arr.append(predicted.item())\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy of the network on the {total} test images: {accuracy:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "a274a6fe-4290-4f59-b523-44060ccde4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "StudentModel                                  --\n",
       "├─Sequential: 1-1                             --\n",
       "│    └─depthwise_separable_conv: 2-1          --\n",
       "│    │    └─Conv2d: 3-1                       21\n",
       "│    │    └─Conv2d: 3-2                       228\n",
       "│    └─BatchNorm2d: 2-2                       128\n",
       "│    └─ReLU: 2-3                              --\n",
       "│    └─depthwise_separable_conv: 2-4          --\n",
       "│    │    └─Conv2d: 3-3                       491\n",
       "│    │    └─Conv2d: 3-4                       1,481\n",
       "│    └─BatchNorm2d: 2-5                       128\n",
       "│    └─ReLU: 2-6                              --\n",
       "│    └─MaxPool2d: 2-7                         --\n",
       "│    └─Dropout: 2-8                           --\n",
       "│    └─depthwise_separable_conv: 2-9          --\n",
       "│    │    └─Conv2d: 3-5                       476\n",
       "│    │    └─Conv2d: 3-6                       2,731\n",
       "│    └─BatchNorm2d: 2-10                      256\n",
       "│    └─ReLU: 2-11                             --\n",
       "│    └─depthwise_separable_conv: 2-12         --\n",
       "│    │    └─Conv2d: 3-7                       957\n",
       "│    │    └─Conv2d: 3-8                       3,889\n",
       "│    └─BatchNorm2d: 2-13                      256\n",
       "│    └─ReLU: 2-14                             --\n",
       "│    └─MaxPool2d: 2-15                        --\n",
       "│    └─Dropout: 2-16                          --\n",
       "│    └─depthwise_separable_conv: 2-17         --\n",
       "│    │    └─Conv2d: 3-9                       938\n",
       "│    │    └─Conv2d: 3-10                      7,907\n",
       "│    └─BatchNorm2d: 2-18                      512\n",
       "│    └─ReLU: 2-19                             --\n",
       "│    └─depthwise_separable_conv: 2-20         --\n",
       "│    │    └─Conv2d: 3-11                      1,893\n",
       "│    │    └─Conv2d: 3-12                      10,897\n",
       "│    └─BatchNorm2d: 2-21                      512\n",
       "│    └─ReLU: 2-22                             --\n",
       "│    └─MaxPool2d: 2-23                        --\n",
       "│    └─Dropout: 2-24                          --\n",
       "│    └─depthwise_separable_conv: 2-25         --\n",
       "│    │    └─Conv2d: 3-13                      2,043\n",
       "│    │    └─Conv2d: 3-14                      11,541\n",
       "│    └─BatchNorm2d: 2-26                      512\n",
       "│    └─ReLU: 2-27                             --\n",
       "│    └─depthwise_separable_conv: 2-28         --\n",
       "│    │    └─Conv2d: 3-15                      2,069\n",
       "│    │    └─Conv2d: 3-16                      22,943\n",
       "│    └─BatchNorm2d: 2-29                      1,024\n",
       "│    └─ReLU: 2-30                             --\n",
       "│    └─AdaptiveAvgPool2d: 2-31                --\n",
       "│    └─Flatten: 2-32                          --\n",
       "├─Sequential: 1-2                             --\n",
       "│    └─Linear: 2-33                           22,756\n",
       "│    └─ReLU: 2-34                             --\n",
       "│    └─Dropout: 2-35                          --\n",
       "│    └─Linear: 2-36                           1,201\n",
       "======================================================================\n",
       "Total params: 97,790\n",
       "Trainable params: 97,790\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.summary(pruned_net.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea77ca5-b09c-44f1-be11-ef837ffcaf0f",
   "metadata": {},
   "source": [
    "### weight_orig是沒有pruned過的參數，weight_mask中0的位置是要prune掉的參數，兩者的product就是pruned過的參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "057063b9-e415-4e6d-b255-215f0abf6a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cnn.0.depthwise.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0.1712, 0.1426, 0.0943], device='cuda:0', requires_grad=True)),\n",
       " ('cnn.0.depthwise.weight_orig',\n",
       "  Parameter containing:\n",
       "  tensor([[[[-0.2440,  0.1404,  0.1166],\n",
       "            [-0.2298,  0.3388,  0.1480],\n",
       "            [-0.0352,  0.1292, -0.0036]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0281, -0.3052, -0.1770],\n",
       "            [ 0.0575, -0.0050,  0.1125],\n",
       "            [ 0.1379,  0.0019, -0.0563]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.2921, -0.0949, -0.0833],\n",
       "            [-0.2350, -0.1615,  0.0462],\n",
       "            [ 0.0827,  0.0352, -0.0306]]]], device='cuda:0', requires_grad=True))]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pruned_net.named_parameters())[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "cdcb46fc-0dd4-4d19-8bac-1fde945de1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cnn.0.depthwise.weight_mask',\n",
       "  tensor([[[[1., 1., 1.],\n",
       "            [1., 1., 1.],\n",
       "            [0., 1., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 1., 1.],\n",
       "            [0., 0., 1.],\n",
       "            [1., 0., 1.]]],\n",
       "  \n",
       "  \n",
       "          [[[1., 1., 1.],\n",
       "            [1., 1., 0.],\n",
       "            [1., 0., 0.]]]], device='cuda:0'))]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pruned_net.named_buffers())[:1] #mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "59da52c8-28da-4353-932b-e2d3cb5cbd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2440,  0.1404,  0.1166],\n",
       "          [-0.2298,  0.3388,  0.1480],\n",
       "          [-0.0000,  0.1292, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, -0.3052, -0.1770],\n",
       "          [ 0.0000, -0.0000,  0.1125],\n",
       "          [ 0.1379,  0.0000, -0.0563]]],\n",
       "\n",
       "\n",
       "        [[[-0.2921, -0.0949, -0.0833],\n",
       "          [-0.2350, -0.1615,  0.0000],\n",
       "          [ 0.0827,  0.0000, -0.0000]]]], device='cuda:0')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_net.cnn[0].depthwise.weight #pruned完的參數"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c01476-5269-468c-9af4-3a3dfc18f7d4",
   "metadata": {},
   "source": [
    "# After removing \"prune mask\", the original parameter will be replaced with pruned parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "c7dec700-4b26-432e-b17f-ee0711509119",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,p in enumerate(param_to_prune):\n",
    "    if p[0]._forward_pre_hooks:\n",
    "        prune.remove(p[0],'weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f19ae75-e433-4415-b3b9-e0a510699817",
   "metadata": {},
   "source": [
    "### Also ,the number of parameters will be restored(97790->474216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "0d14b8d4-9eda-4573-aa26-c05c6e094771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "StudentModel                                  --\n",
       "├─Sequential: 1-1                             --\n",
       "│    └─depthwise_separable_conv: 2-1          --\n",
       "│    │    └─Conv2d: 3-1                       30\n",
       "│    │    └─Conv2d: 3-2                       256\n",
       "│    └─BatchNorm2d: 2-2                       128\n",
       "│    └─ReLU: 2-3                              --\n",
       "│    └─depthwise_separable_conv: 2-4          --\n",
       "│    │    └─Conv2d: 3-3                       640\n",
       "│    │    └─Conv2d: 3-4                       4,160\n",
       "│    └─BatchNorm2d: 2-5                       128\n",
       "│    └─ReLU: 2-6                              --\n",
       "│    └─MaxPool2d: 2-7                         --\n",
       "│    └─Dropout: 2-8                           --\n",
       "│    └─depthwise_separable_conv: 2-9          --\n",
       "│    │    └─Conv2d: 3-5                       640\n",
       "│    │    └─Conv2d: 3-6                       8,320\n",
       "│    └─BatchNorm2d: 2-10                      256\n",
       "│    └─ReLU: 2-11                             --\n",
       "│    └─depthwise_separable_conv: 2-12         --\n",
       "│    │    └─Conv2d: 3-7                       1,280\n",
       "│    │    └─Conv2d: 3-8                       16,512\n",
       "│    └─BatchNorm2d: 2-13                      256\n",
       "│    └─ReLU: 2-14                             --\n",
       "│    └─MaxPool2d: 2-15                        --\n",
       "│    └─Dropout: 2-16                          --\n",
       "│    └─depthwise_separable_conv: 2-17         --\n",
       "│    │    └─Conv2d: 3-9                       1,280\n",
       "│    │    └─Conv2d: 3-10                      33,024\n",
       "│    └─BatchNorm2d: 2-18                      512\n",
       "│    └─ReLU: 2-19                             --\n",
       "│    └─depthwise_separable_conv: 2-20         --\n",
       "│    │    └─Conv2d: 3-11                      2,560\n",
       "│    │    └─Conv2d: 3-12                      65,792\n",
       "│    └─BatchNorm2d: 2-21                      512\n",
       "│    └─ReLU: 2-22                             --\n",
       "│    └─MaxPool2d: 2-23                        --\n",
       "│    └─Dropout: 2-24                          --\n",
       "│    └─depthwise_separable_conv: 2-25         --\n",
       "│    │    └─Conv2d: 3-13                      2,560\n",
       "│    │    └─Conv2d: 3-14                      65,792\n",
       "│    └─BatchNorm2d: 2-26                      512\n",
       "│    └─ReLU: 2-27                             --\n",
       "│    └─depthwise_separable_conv: 2-28         --\n",
       "│    │    └─Conv2d: 3-15                      2,560\n",
       "│    │    └─Conv2d: 3-16                      131,584\n",
       "│    └─BatchNorm2d: 2-29                      1,024\n",
       "│    └─ReLU: 2-30                             --\n",
       "│    └─AdaptiveAvgPool2d: 2-31                --\n",
       "│    └─Flatten: 2-32                          --\n",
       "├─Sequential: 1-2                             --\n",
       "│    └─Linear: 2-33                           131,328\n",
       "│    └─ReLU: 2-34                             --\n",
       "│    └─Dropout: 2-35                          --\n",
       "│    └─Linear: 2-36                           2,570\n",
       "======================================================================\n",
       "Total params: 474,216\n",
       "Trainable params: 474,216\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo as summary\n",
    "summary.summary(pruned_net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-model_compression]",
   "language": "python",
   "name": "conda-env-.conda-model_compression-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
