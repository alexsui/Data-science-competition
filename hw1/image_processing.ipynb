{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# collect data from ppt beauty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "import sys\n",
    "payload = {\n",
    "    'from':'/bbs/Beauty/index.html',\n",
    "    'yes':'yes'\n",
    "}\n",
    "rs = requests.session()\n",
    "res = rs.post(\"https://www.ptt.cc/ask/over18\",data = payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_post_index():\n",
    "    i=1\n",
    "    while True:\n",
    "        content = rs.get(f\"https://www.ptt.cc/bbs/Beauty/index{i}.html\")\n",
    "        soup = BeautifulSoup(content.text, 'html.parser')\n",
    "        articles = soup.select('div.r-ent')\n",
    "        for article in articles:\n",
    "            title = article.find(\"div\", class_=\"title\")\n",
    "            if title.a is None:\n",
    "                continue\n",
    "            url = str(title.a['href'])\n",
    "            if url == \"/bbs/Beauty/M.1640974182.A.7DB.html\":\n",
    "                return i\n",
    "        i+=1\n",
    "def get_last_post_index(start_index):\n",
    "    index = start_index\n",
    "    tmp_date =\"\"\n",
    "    while True:\n",
    "        content = rs.get(f\"https://www.ptt.cc/bbs/Beauty/index{index}.html\")\n",
    "        soup = BeautifulSoup(content.text, 'html.parser')\n",
    "        articles = soup.select('div.r-ent')\n",
    "        for article in articles:\n",
    "            date = article.select_one(\"div.date\").text.strip()\n",
    "            if (tmp_date ==\"\")&(date==\"1/01\"):\n",
    "                tmp_date = date\n",
    "            if (date==\"1/01\") & (tmp_date ==\"12/31\"):\n",
    "                return index\n",
    "            if tmp_date !=\"\":\n",
    "                tmp_date = date\n",
    "        index += 1\n",
    "start_index = get_first_post_index()\n",
    "end_index = get_last_post_index(start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3642, 3951)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index,end_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1(score 0~100) :\n",
    "* Get image link from ptt\n",
    "* 以推文數作為image score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_score = {i : [] for i in range(0,101)}\n",
    "lock = threading.Lock()\n",
    "def get_image(index):\n",
    "    global image_score, lock\n",
    "    content = rs.get(f\"https://www.ptt.cc/bbs/Beauty/index{index}.html\")\n",
    "    soup = BeautifulSoup(content.text, 'html.parser')\n",
    "    articles = soup.select('div.r-ent')\n",
    "    for article in articles:\n",
    "        article_tag = article.select_one(\".nrec\").text\n",
    "        if article_tag==\"\":\n",
    "            continue\n",
    "        url = \"https://www.ptt.cc/\" +article.select_one('.title').a['href']\n",
    "        content = rs.get(url)\n",
    "        soup = BeautifulSoup(content.text, 'html.parser')\n",
    "        main = soup.select_one(\"div#main-content\")\n",
    "        excluded_tag = main.select(\".push,.f2,div.article-mataline,.article-mataline-right\")\n",
    "        for tag in excluded_tag:\n",
    "            tag.decompose()\n",
    "        all_link = main.select(\"a\")\n",
    "        for link in all_link:\n",
    "            if re.match(r'.*\\.(jpg|png|jpeg)$',link['href']):\n",
    "                with lock:\n",
    "                    if article_tag==\"爆\":\n",
    "                        image_score[100].append(link['href'])\n",
    "                    elif not article_tag.isnumeric():\n",
    "                        image_score[0].append(link['href'])\n",
    "                    else:\n",
    "                        image_score[int(article_tag)].append(link['href'])\n",
    "        delay_choices = [0.2,0.3,0.4,0.5] \n",
    "        delay = random.choice(delay_choices) \n",
    "        time.sleep(delay)\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for i in range(2900,4002):\n",
    "        executor.submit(get_image,i)\n",
    "output1 = \"image_score.json\"\n",
    "# output2 = \"not_popular_image.json\"\n",
    "with open(output1,'w') as f:\n",
    "    json.dump(image_score,f)\n",
    "# with open(output2,'w') as f:\n",
    "#     json.dump({\n",
    "#         \"image_urls\":not_popular_image\n",
    "#     },f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convert link to image file\n",
    "* check if image can be read by Image.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import os\n",
    "with open(\"./image_score.json\") as f:\n",
    "    image_score = json.load(f)\n",
    "for i in range(1,101):\n",
    "    data = image_score[i]\n",
    "    folder_path = f\"data/{i}/\"\n",
    "    Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "    for i in range(len(data)):\n",
    "        try:\n",
    "            url = data[i]\n",
    "            filename = folder_path/data[i].split(\"/\")[-1]\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "        except Exception as err:\n",
    "            print(f\"Sth. wrong with {url}, error : {err}\")\n",
    "for i in range(1,101):\n",
    "    image_list = list(Path(\"./data/{i}\").glob(\"*\"))\n",
    "    for image in image_list:\n",
    "        try:\n",
    "            img = Image.open(image)\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            os.remove(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* split data into train & test data(from data folder to new_data folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for i in range(0,101):\n",
    "    image_list = list(Path(\"./data/{i}\").glob(\"*\"))\n",
    "    for k,image in enumerate(image_list):\n",
    "        if k>len(image_list)/5:\n",
    "            shutil.copy(image,\"./new_data/train/{i}/\")\n",
    "        else:\n",
    "            shutil.mocopyve(image,\"./new_data/test/{i}/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* crop the face from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "import os \n",
    "\n",
    "for i in range(0,101):\n",
    "    train_image_list = list(Path(f\"new_data/train/{i}/\").glob(\"*\"))\n",
    "    test_image_list = list(Path(f\"new_data/test/{i}/\").glob(\"*\"))\n",
    "    for l in [train_image_list,test_image_list]:\n",
    "        for image in l:\n",
    "            # print(image)\n",
    "            img = face_recognition.load_image_file(image)\n",
    "            face_locations = face_recognition.face_locations(img)\n",
    "            if len(face_locations) != 1 :\n",
    "                # print(\"not a good image\")\n",
    "                os.remove(image)\n",
    "                continue\n",
    "            else:\n",
    "                top, right, bottom, left = face_locations[0]\n",
    "                face_img = img[top:bottom, left:right]\n",
    "                pil_image = Image.fromarray(face_img)\n",
    "                pil_image.save(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2(popular & not popular) :\n",
    "* modify method 1's code to scrape link from ptt (get popular_image.json & not_popular_image.json)\n",
    "* Split data into train & test\n",
    "* convert link to image file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sth. wrong with https://imgur.dcard.tw/IIwP9Y4h.png, error : HTTP Error 403: Forbidden\n",
      "Sth. wrong with https://imgur.com/undefined.jpg, error : HTTP Error 404: Not Found\n",
      "Sth. wrong with https://i2.kknews.cc/SIG=1mjs6kc/1ns40000q02nopp1n5s5.jpg, error : HTTP Error 403: Forbidden\n",
      "Sth. wrong with https://i1.kknews.cc/SIG=318r8na/19sr00069o3rq6so2706.jpg, error : HTTP Error 403: Forbidden\n",
      "Sth. wrong with https://img.bingfeng.tw/bingfeng/forum/201811/19/210508q7x70trrmvsmotjr.jpg, error : HTTP Error 403: Forbidden\n",
      "Sth. wrong with https://assets.juksy.com/files/articles/91916/800x_100_w-5d11dea5c11ba.png, error : HTTP Error 403: Forbidden\n",
      "Sth. wrong with https://file.jichengguandao.com/uploads/2020/07/07/nzuxf25ot0m.png, error : <urlopen error [WinError 10054] 遠端主機已強制關閉一個現存的連線。>\n",
      "Sth. wrong with https://img.chinatimes.com/newsphoto/2019-05-08/656/20190508001595.jpg, error : <urlopen error [Errno 11001] getaddrinfo failed>\n",
      "Sth. wrong with https://img.chinatimes.com/newsphoto/2019-05-08/656/20190508001603.jpg, error : <urlopen error [Errno 11001] getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from urllib.error import HTTPError\n",
    "with open(\"./popular_image.json\") as f:\n",
    "    popular_image = json.load(f)\n",
    "with open(\"./not_popular_image.json\") as f:\n",
    "    not_popular_image = json.load(f)\n",
    "not_popular_image = not_popular_image['image_urls'][:15000]\n",
    "popular_image = popular_image['image_urls'][:15000]\n",
    "\n",
    "for i in range(len(popular_image)):\n",
    "    if i<(len(popular_image)*4/5):\n",
    "        try:\n",
    "            url = popular_image[i]\n",
    "            filename = \"train/popular/\"+popular_image[i].split(\"/\")[-1]\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "        except Exception as err:\n",
    "            print(f\"Sth. wrong with {url}, error : {err}\")\n",
    "        # continue\n",
    "    else:\n",
    "        try:\n",
    "            url = popular_image[i]\n",
    "            filename = \"test/popular/\"+popular_image[i].split(\"/\")[-1]\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "        except Exception as err:\n",
    "            print(f\"Sth. wrong with {url}, error : {err}\")\n",
    "for i in range(len(not_popular_image)):\n",
    "    if i<(len(not_popular_image)*4/5):\n",
    "        try:\n",
    "            url = not_popular_image[i]\n",
    "            filename = \"train/not_popular/\" + not_popular_image[i].split(\"/\")[-1]\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "        except Exception as err:\n",
    "            print(f\"Sth. wrong with {url}, error : {err}\")\n",
    "            \n",
    "    else:\n",
    "        try:\n",
    "            url = not_popular_image[i]\n",
    "            filename = \"test/not_popular/\"+not_popular_image[i].split(\"/\")[-1]\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "        except Exception as err:\n",
    "            print(f\"Sth. wrong with {url}, error : {err}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* get number of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train popular: 11807\n",
      "train not_popular: 11522\n",
      "test popular: 2985\n",
      "test not_popular: 2980\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "mode = [\"train\", \"test\"]\n",
    "t= [\"popular\", \"not_popular\"]\n",
    "for m in mode:\n",
    "    for i in t:\n",
    "        folder = f\"./{m}/{i}/\"\n",
    "        data = glob.glob(os.path.join(folder,'*'))\n",
    "        print(f\"{m} {i}:\",len(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c732b51fddb1f99251154ad9dab8b45b41ae5f2e2750db0612b112621aa0703"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
